{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oDBeMnbDLou"
      },
      "outputs": [],
      "source": [
        "# Understanding concepts of using Pre-Trained models for feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The key concepts and steps involved in using a pre-trained MobileNet V2 model\n",
        "# for feature extraction in deep learning\n",
        "\n",
        "\"\"\"\n",
        "MobileNet V2:\n",
        " MobileNetV2 is a convolutional neural network architecture designed for efficient\n",
        " and lightweight deep learning tasks. It's particularly useful for mobile and\n",
        " embedded applications due to its compact design while maintaining respectable accuracy.\n",
        " The architecture contains multiple layers, including depthwise separable convolutions,\n",
        " that allow it to achieve a good trade-off between model size and performance.\n",
        "\n",
        "\n",
        "Feature Extraction:\n",
        " Feature extraction involves using a pre-trained deep learning model to capture\n",
        " meaningful features from input data. Instead of training the entire model from scratch,\n",
        " you leverage the knowledge already acquired by the model during its original\n",
        " training on a large dataset.\n",
        "\n",
        "\n",
        "Bottleneck Layer:\n",
        " The \"bottleneck layer\" refers to the last convolutional layer before the final\n",
        " classification layers in the MobileNet V2 architecture. This layer is called a\n",
        " \"bottleneck\" because it represents a condensed version of the feature map before\n",
        " it's flattened and passed to the fully connected layers for classification.\n",
        " The features in this layer are more general and abstract compared to the final\n",
        " classification layer, making them useful for various tasks beyond the specific\n",
        " classes in ImageNet.\n",
        "\n",
        "\n",
        "Include Top Argument:\n",
        " When you instantiate a MobileNet V2 model using a deep learning framework like Keras,\n",
        " you have the option to specify the include_top argument. By setting include_top=False,\n",
        " you're indicating that you want to exclude the final classification layers from the model.\n",
        " This is commonly done when using the model for feature extraction, as you're interested\n",
        " in the intermediate features rather than the final classification.\n",
        "\n",
        "\n",
        "Instantiating Pre-trained Model:\n",
        " The next step involves creating an instance of the MobileNet V2 model pre-loaded\n",
        " with weights that were trained on the ImageNet dataset. These pre-trained weights\n",
        " contain knowledge about various features present in different images.\n",
        " By leveraging these weights, you save time and resources that would have been\n",
        " required to train the model from scratch.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "TvbcAyF-EBgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Some important concepts related to BatchNormalization layers in the context\n",
        "# of fine-tuning a deep learning model\n",
        "\n",
        "\"\"\"\n",
        "BatchNormalization Layer:\n",
        " BatchNormalization is a technique used in deep neural networks to stabilize and\n",
        " accelerate training. It helps mitigate issues like internal covariate shift by\n",
        " normalizing the activations of a layer across mini-batches during training.\n",
        " This leads to faster convergence and better generalization.\n",
        "\n",
        "\n",
        "Fine-Tuning:\n",
        " Fine-tuning involves taking a pre-trained neural network (often on a large dataset)\n",
        " and adapting it to a new task or dataset. Instead of training the entire network\n",
        " from scratch, you adjust the existing weights to make them suitable for the new problem.\n",
        "\n",
        "\n",
        "layer.trainable = False:\n",
        " In the context of fine-tuning, you might want to freeze certain layers (make them\n",
        " non-trainable) in the pre-trained model to avoid overfitting or losing the learned\n",
        " features. By setting layer.trainable = False for specific layers, you prevent\n",
        " them from being updated during training.\n",
        "\n",
        "\n",
        "Inference Mode for BatchNormalization:\n",
        " When you set a BatchNormalization layer's trainable attribute to False, it switches\n",
        " to inference mode during training. In this mode, the BatchNormalization layer uses\n",
        " the previously computed mean and variance statistics to normalize activations.\n",
        " This is important because updating mean and variance statistics during fine-tuning\n",
        " could lead to instability.\n",
        "\n",
        "\n",
        "Unfreezing and Inference Mode:\n",
        " When you decide to unfreeze (make trainable) certain layers in a model for fine-tuning,\n",
        " including BatchNormalization layers, it's crucial to ensure that the BatchNormalization\n",
        " layers stay in inference mode. This is done by explicitly passing training=False when\n",
        " calling the base model. If you fail to do this and let BatchNormalization layers update\n",
        " their statistics during fine-tuning, it could disrupt the knowledge learned by the model so far.\n",
        "\n",
        " \"\"\""
      ],
      "metadata": {
        "id": "-rey9Dx9FKk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fetching the pre-trained model, while not including top layer\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[160, 160, 3],\n",
        "                                               include_top=False, # <---\n",
        "                                               weights='imagenet')\n",
        "\n",
        "# setting the trainable parameter of pre-trained model to False\n",
        "base_model.trainable = False\n",
        "\n",
        "# using preprocessing layer of pre-trained model\n",
        "preprocess_layer = tf.keras.applications.mobilenet_v2.preprocess_input(inputs)\n",
        "\n",
        "# set the trainable parameter to False for layers to be freezed while training\n",
        "model = base_model(preprocess_layer, training = False)\n",
        "\n",
        "# the above code will extract all the feature from the training image\n",
        "# and output will be based on last layers except top of the pre-trained model\n",
        "# e.g : [ batch_size, 5, 5, 1280]"
      ],
      "metadata": {
        "id": "vaYIXBUPGBYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# How to add a classification head to a feature block generated by a neural network.\n",
        "# Head layer should be added as per custom problem with image data and number of classes\n",
        "\n",
        "\"\"\"\n",
        "Classification Head:\n",
        " A classification head is the final part of a neural network used to generate\n",
        " predictions for a specific task, such as image classification. It takes the\n",
        " extracted features from the previous layers and maps them to the target classes.\n",
        "\n",
        "\n",
        "Global Average Pooling:\n",
        " Before passing the extracted features to the classification head, it's common to\n",
        " apply a global average pooling operation. This operation reduces the spatial\n",
        " dimensions of the feature maps while retaining the channel information. It involves\n",
        " taking the average of all the values in each channel, resulting in a single value\n",
        " for each channel. This reduces the overall number of parameters and provides a\n",
        " more compact representation.\n",
        "\n",
        "\n",
        "tf.keras.layers.GlobalAveragePooling2D:\n",
        " tf.keras.layers.GlobalAveragePooling2D is a layer in TensorFlow that performs\n",
        " global average pooling on the feature maps. It converts the 2D spatial grid of\n",
        " values into a single value per channel. This is particularly useful when you have\n",
        " variable input sizes since it produces a fixed-size output regardless of the input size.\n",
        "\n",
        "\n",
        "Dense Layer for Classification:\n",
        " After applying global average pooling, you use a tf.keras.layers.Dense layer to\n",
        " transform the pooled features into class predictions. This layer connects every\n",
        " element of the input vector to every output unit (class). It learns the weights\n",
        " that best map the features to class predictions.\n",
        "\n",
        "\n",
        "Logit:\n",
        " The output of the tf.keras.layers.Dense layer is often referred to as logits.\n",
        " Logits are unnormalized prediction values. Positive logits indicate a higher\n",
        " likelihood of the presence of a certain class, while negative logits indicate a\n",
        " higher likelihood of the absence of a class. These logits will later be\n",
        " transformed using a softmax function to produce class probabilities.\n",
        "\n",
        "\"\"\"\"\n",
        "\n",
        "Activation Function for Logits:\n",
        " In the provided content, it's mentioned that you don't need an activation function\n",
        " after the tf.keras.layers.Dense layer. This is because the model will treat these\n",
        " predictions as raw logits. These logits will be passed through a softmax activation\n",
        " function during loss calculation to transform them into class probabilities."
      ],
      "metadata": {
        "id": "js7Cx-F9Hi3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Classification Head layers based on problem statement\n",
        "\n",
        "# performs global average pooling on the feature maps. It converts the 2D spatial\n",
        "# grid of values into a single value per channel.\n",
        "avg_pool_layer = tf.keras.layers.GlobalAveragePooling2D()(model_layer)\n",
        "\n",
        "# Dropout layer to avoid overfitting\n",
        "dropout_layer = tf.keras.layers.Dropout(0.2)(avg_pool_layer)\n",
        "\n",
        "# This layer connects every element of the input vector to every output unit (class)\n",
        "# this is also called Logits (unnormalized prediction values)\n",
        "output = tf.keras.layers.Dense(output_channels)(dropout_layer)\n",
        "\n",
        "# this output value will be logits which further needs to be transformed using\n",
        "# softmax function to produce class probablities"
      ],
      "metadata": {
        "id": "0cEqrB-KHiw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating pre-trained model (base model) with validation dataset\n",
        "\n",
        "loss_0, accuracy_0 = model.evaluate(validation_dataset)\n",
        "\n",
        "# this will give the base accuracy and loss without training the model\n",
        "# with our custom data"
      ],
      "metadata": {
        "id": "aFk0T13bKmJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training the model, We can predict through model with test data\n",
        "\n",
        "#Retrieve a batch of images from the test set\n",
        "for img, label in test_dataset.take(1):\n",
        "  image_batch, label_batch = img, label\n",
        "\n",
        "# do the predictions on batch data\n",
        "predictions = model.predict_on_batch(image_batch).flatten()\n",
        "\n",
        "# Apply a sigmoid since our model returns logits\n",
        "predictions = tf.nn.sigmoid(predictions) # as it has only 2 classes\n",
        "predictions = tf.where(predictions < 0.5, 0, 1)\n",
        "\n",
        "print('Predictions:\\n', predictions.numpy())\n",
        "print('Labels:\\n', label_batch)"
      ],
      "metadata": {
        "id": "zOe_1igLK8-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Why validation metrics might appear better than training metrics in some cases\n",
        "# during the training of a deep learning model\n",
        "\n",
        "\"\"\"\n",
        "BatchNormalization and Dropout:\n",
        "\n",
        "  -> tf.keras.layers.BatchNormalization and tf.keras.layers.Dropout are regularization\n",
        "     techniques used in neural networks to improve generalization and prevent overfitting.\n",
        "  -> BatchNormalization normalizes the activations of a layer, which helps stabilize\n",
        "     training by reducing internal covariate shift.\n",
        "  -> Dropout randomly drops a fraction of the neurons during training, effectively\n",
        "     preventing the network from relying too much on any single neuron.\n",
        "\n",
        "\n",
        "Validation Metrics vs Training Metrics:\n",
        "\n",
        "  -> During training, layers like BatchNormalization and Dropout are active, affecting\n",
        "     the behavior of the network. This means that the training metrics (such as accuracy)\n",
        "     calculated during training may be influenced by these layers.\n",
        "  -> However, when evaluating the model on the validation set, these layers are\n",
        "     typically turned off. This is because during validation or testing, you want a\n",
        "     more representative evaluation of the model's generalization performance without\n",
        "     the influence of Dropout or BatchNormalization.\n",
        "  -> As a result, the validation metrics might appear better compared to the training\n",
        "     metrics because the validation set evaluates the model under more controlled\n",
        "     conditions without the regularization effects\n",
        "\n",
        "Epoch-Level Averaging:\n",
        "\n",
        "  -> Training metrics are often reported as averages over the course of an epoch.\n",
        "    An epoch is completed when all training samples have been used once for training.\n",
        "  -> Validation metrics, on the other hand, are evaluated at the end of each epoch\n",
        "     after the model has seen the entire training dataset.\n",
        "  -> Due to this timing difference, the validation metrics are evaluated on a slightly\n",
        "     more trained model compared to the metrics reported during training.\n",
        "  -> This difference in timing could contribute to the validation metrics appearing\n",
        "     slightly better than the training metrics.\n",
        "\"\"\"\n",
        "\n",
        "# The content explains that the difference in behavior between training and validation\n",
        "# metrics is influenced by the presence of regularization techniques like\n",
        "# BatchNormalization and Dropout."
      ],
      "metadata": {
        "id": "shroXi-fL3Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Benefits of Using a pre-trained model for feature extraction:\n",
        "\n",
        "# 1. It leverages the knowledge and features learned from a larger dataset, which\n",
        "#    can improve performance on the small dataset.\n",
        "# 2. It reduces the risk of overfitting by using features that are more generic\n",
        "#    and transferable.\n",
        "# 3. It requires fewer training iterations since the convolutional base is frozen,\n",
        "#    saving both time and computational resources."
      ],
      "metadata": {
        "id": "vBG6S1IoNO6N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}